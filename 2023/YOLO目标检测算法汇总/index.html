<!DOCTYPE html><html lang="zh-CN"><head><meta charset="utf-8"><meta name="keywords" content="学习, 技术, 随笔, 摄影, 生活"><meta name="viewport" content="width=device-width,initial-scale=1,maximum-scale=1"><title>YOLO 目标检测算法汇总 | SongXJ&#39;s Blog</title><meta name="generator" content="hexo-theme-ayer"><link rel="shortcut icon" href="/SongXJ01/images/songxj.ico"><link rel="stylesheet" href="/SongXJ01/dist/main.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/Shen-Yu/cdn/css/remixicon.min.css"><link rel="stylesheet" href="/SongXJ01/css/custom.css"><script src="https://cdn.jsdelivr.net/npm/pace-js@1.0.2/pace.min.js"></script><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@sweetalert2/theme-bulma@5.0.1/bulma.min.css"><script src="https://cdn.jsdelivr.net/npm/sweetalert2@11.0.19/dist/sweetalert2.min.js"></script><script src="https://cdn.jsdelivr.net/npm/mermaid@8.9.2/dist/mermaid.min.js"></script><style>.swal2-styled.swal2-confirm{font-size:1.6rem}</style><link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/Zfour/Butterfly-swiper/swiper/swiper.min.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/Zfour/Butterfly-swiper/swiper/swiperstyle.css"><link rel="alternate" href="/SongXJ01/atom.xml" title="SongXJ's Blog" type="application/atom+xml"><link href="https://cdn.bootcss.com/KaTeX/0.11.1/katex.min.css" rel="stylesheet"></head></html><body><div id="app"><main class="content on"><section class="outer"><article id="post-YOLO目标检测算法汇总" class="article article-type-post" itemscope itemprop="blogPost" data-scroll-reveal><div class="article-inner"><header class="article-header"><h1 class="article-title sea-center" style="border-left:0" itemprop="name">YOLO 目标检测算法汇总</h1></header><div class="article-meta"><a href="/SongXJ01/2023/YOLO%E7%9B%AE%E6%A0%87%E6%A3%80%E6%B5%8B%E7%AE%97%E6%B3%95%E6%B1%87%E6%80%BB/" class="article-date"><time datetime="2023-08-09T08:36:01.000Z" itemprop="datePublished">2023-08-09</time></a><div class="article-category"><a class="article-category-link" href="/SongXJ01/categories/%E6%8A%80%E6%9C%AF%E7%AC%94%E8%AE%B0/">技术笔记</a> / <a class="article-category-link" href="/SongXJ01/categories/%E6%8A%80%E6%9C%AF%E7%AC%94%E8%AE%B0/%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89/">计算机视觉</a></div><div class="word_count"><span class="post-time"><span class="post-meta-item-icon"><i class="ri-quill-pen-line"></i> <span class="post-meta-item-text">字数统计:</span> <span class="post-count">7k</span> </span></span><span class="post-time">&nbsp; | &nbsp; <span class="post-meta-item-icon"><i class="ri-book-open-line"></i> <span class="post-meta-item-text">阅读时长≈</span> <span class="post-count">25 分钟</span></span></span></div></div><div class="tocbot"></div><div class="article-entry" itemprop="articleBody"><p>  YOLO（You Only Look Once）是一系列目标检测算法，其核心思想是将目标检测视为回归问题，从而实现了端到端的训练。从最初的 YOLOv1 开始，该系列算法在速度和准确性方面不断取得突破。YOLOv1 引入了单网络结构，实现了端到端的训练，虽然定位精度有待提高，但其高速性能为后续版本奠定了基础。YOLOv2（YOLO9000）通过引入批归一化、高分辨率分类器、锚点框等改进，显著提升了检测性能。YOLOv3 则在保持高速的同时，通过多尺度预测、更好的基础分类网络和对象分类损失函数等优化，进一步提高了检测精度。而最新的 YOLOv4 和 YOLOv5 则在保持 YOLO 系列一贯的高速特性的同时，通过引入各种新的技巧和模块，如 CSPNet、PANet、SiLU 激活函数等，进一步提升了检测精度和速度。总体而言，YOLO 系列算法以其高效、准确的特点，在目标检测领域持续引领创新，成为实际应用中的热门选择。</p><span id="more"></span><p><br><br></p><hr><br><h2 id="基本知识">基本知识</h2><h4 id="目标检测经典算法">目标检测经典算法</h4><ul><li>两阶段：Faster-RCNN，Mask-RCNN 系列</li><li>单阶段：YOLO 系列<ul><li>优势：速度快，实时监测</li></ul></li></ul><h4 id="衡量指标">衡量指标</h4><ul><li><p>mAP：越大，效果越好。（面积之和）</p><p><img src="/SongXJ01/images/YOLO%E7%9B%AE%E6%A0%87%E6%A3%80%E6%B5%8B%E7%AE%97%E6%B3%95%E6%B1%87%E6%80%BB/image_oVjRjsHkrR.png" alt=""></p></li><li><p>IoU：预测框与 ground truth 的交集和并集的比值。</p><p><img src="/SongXJ01/images/YOLO%E7%9B%AE%E6%A0%87%E6%A3%80%E6%B5%8B%E7%AE%97%E6%B3%95%E6%B1%87%E6%80%BB/image_tUSzCO6bWA.png" alt=""></p></li><li><p>FPS：帧率，越大，速度越快，网络结构越简单</p></li><li><p>P 和 R：</p><p><img src="/SongXJ01/images/YOLO%E7%9B%AE%E6%A0%87%E6%A3%80%E6%B5%8B%E7%AE%97%E6%B3%95%E6%B1%87%E6%80%BB/image_4aqpOax2km.png" alt=""></p></li></ul><p><br><br></p><hr><br><h2 id="发展历程">发展历程</h2><p><img src="/SongXJ01/images/YOLO%E7%9B%AE%E6%A0%87%E6%A3%80%E6%B5%8B%E7%AE%97%E6%B3%95%E6%B1%87%E6%80%BB/image_-ZryUIZcpe.png" alt="发展历程"></p><p><br><br></p><h2 id="yolov1">YOLOv1</h2><h3 id="概述">概述</h3><p>  以往的二阶段检测算法，例如 Faster-RCNN，在检测时需要经过两步：边框回归和 softmax 分类。由于大量预选框的生成，该方法检测精度较高，但实时性较差。鉴于此，YOLO 之父 Joseph Redmon 创新性的提出了通过直接回归的方式获取目标检测的具体位置信息和类别分类信息，极大的降低了计算量，显著提升了检测的速度，达到了 45FPS。</p><h3 id="核心思想"><strong>核心思想</strong></h3><p>  将整张图作为网络的输入，直接在输出层对 BBox 的位置和类别进行回归。</p><ol><li>给个一个输入图像，首先将图像划分成 7 × 7 的网格</li><li>对于每个网格，我们都预测 2 个边框（包括每个边框是目标的置信度以及每个边框区域在多个类别上的概率）</li><li>根据上一步可以预测出 7 × 7 × 2 个目标窗口，然后根据阈值去除可能性比较低的目标窗口，最后 NMS 去除冗余窗口即可</li></ol><h3 id="优缺点">优缺点</h3><p><strong>优点：</strong></p><ul><li>快速，pipline 简单；</li><li>背景误检率低；</li><li>通用性强。</li></ul><p><strong>缺点：</strong></p><ul><li><strong>对于小而密集的物体检测效率低：</strong> 由于输出层为全连接层，因此在检测时，YOLO v1 训练模型只支持与训练图像相同的输入分辨率。虽然每个格子可以预测 几个 bounding box，但是最终只选择只选择 IOU 最高的 bounding box 作为物体检测输出，即每个格子最多只预测出一个物体。当物体占画面比例较小，如图像中包含畜群或鸟群时，每个格子包含多个物体，但却只能检测出其中一个。</li></ul><p><br><br></p><h2 id="yolov2">YOLOv2</h2><h3 id="概述">概述</h3><p>  针对 YOLOv1 的问题，YOLO 之父 Joseph Redmon 不甘屈服，对 v1 版本进行了大刀阔斧的改革，继而提出了 YOLOv2 网络，重要改革举措包括：</p><ol><li>更换骨干网络；</li><li>引入 PassThrough;</li><li>借鉴二阶段检测的思想，添加了预选框。</li></ol><p>  YOLOv2 检测算法是将图片输入到 darknet19 网络中提取特征图，然后输出目标框类别信息和位置信息。</p><h3 id="passthrough 操作">PassThrough 操作</h3><p>  该方法将 28x28x512 调整为 14x14x2048，后续 v5 版本中的 <strong>Focus</strong> 操作类似该操作。将生成的 14x14x2048 与原始的 14x14x1024 进行 concat 操作。</p><p><img src="/SongXJ01/images/YOLO%E7%9B%AE%E6%A0%87%E6%A3%80%E6%B5%8B%E7%AE%97%E6%B3%95%E6%B1%87%E6%80%BB/image_36lEwKsrgk.png" alt=""></p><h3 id="引入 anchor">引入 anchor</h3><p>  <strong>引入 anchor，调整位置预测为偏移量预测</strong> 借鉴了 Faster-RCNN 的思想，引入了 anchor，将目标框的位置预测由直接预测坐标调整为偏移量预测，大大降低了预测难度，提升了预测准确性。</p><p><img src="/SongXJ01/images/YOLO%E7%9B%AE%E6%A0%87%E6%A3%80%E6%B5%8B%E7%AE%97%E6%B3%95%E6%B1%87%E6%80%BB/image_O18zvUQ-HJ.png" alt=""></p><h3 id="优缺点">优缺点</h3><p>（1）优势：利用 passthrough 操作对高低层语义信息进行融合，在一定程度上增强了小目标的检测能力。采用小卷积核替代 7x7 大卷积核，降低了计算量。同时改进的位置偏移的策略降低了检测目标框的难度。</p><p>（2）不足：尚未采用残差网络结构。且当存在多物体密集挨着的时候或者小目标的时候，检测效果有待提升。</p><p><br><br></p><h2 id="yolov3">YOLOv3</h2><p>  针对 YOLOv2 的问题，YOLO 之父 Joseph Redmon 决定深化改革，于是乎吸收当下较好的网络设计思想，引入了 <strong>残差网络模块</strong>。基本解决了小目标检测的问题，在速度和精度上实现了较好的平衡。</p><ol><li>在 Darknet19 的基础上推陈出新，引入残差，并加深网络深度，提出了 Darkent53（类似于 ResNet 引入残差结构）。</li><li>借鉴了 <strong>特征金字塔 </strong>的思想，在三个不同的尺寸上分别进行预测（多尺度预测 ，引入 FPN）。</li><li>分类器不再使用 Softmax，分类损失采用 binary cross-entropy loss（二分类交叉损失熵）</li></ol><p><img src="/SongXJ01/images/YOLO%E7%9B%AE%E6%A0%87%E6%A3%80%E6%B5%8B%E7%AE%97%E6%B3%95%E6%B1%87%E6%80%BB/image_OFLllUQUbS.png" alt="YOLOv3 网络结构图"></p><h3 id="特征金字塔">特征金字塔</h3><p>  YOLOv3 检测算法是将图片输入到 darknet53 网络中提取特征图，然后借鉴特征金字塔网络思想，将高级和低级语义信息进行融合，在低、中、高三个层次上分别预测目标框，最后输出三个尺度的特征图信息（52×52×75、26×26×75、13×13×75）。</p><p><br><br></p><h2 id="yolov4">YOLOv4</h2><p>YOLOv4 就是 <strong>筛选 </strong>了一些从 YOLOv3 发布至今，能够<strong> 提高检测精度 </strong>的<strong>tricks</strong>，并以 YOLOv3 为 <strong>基础 </strong>进行改进。</p><ul><li>相较于 YOLOv3 的 DarkNet53，YOLOv4 用了 CSPDarkNet53</li><li>相较于 YOLOv3 的 FPN，YOLOv4 用了<strong>SPP</strong>+PAN</li><li>CutMix 数据增强和马赛克（Mosaic）数据增强</li><li>DropBlock 正则化</li></ul><p><img src="/SongXJ01/images/YOLO%E7%9B%AE%E6%A0%87%E6%A3%80%E6%B5%8B%E7%AE%97%E6%B3%95%E6%B1%87%E6%80%BB/image_eDki8sf-KG.png" alt="YOLOv4 网络结构图"></p><h3 id="输入数据采用 mosaic 数据增强">输入数据采用 Mosaic 数据增强</h3><p>  借鉴了 2019 年 CutMix 的思路，并在此基础上进行了拓展，Mosaic 数据增强方式采用了 4 张图片，随机缩放、随机裁剪、随机排布的方式进行拼接。从而对小目标的检测起到进一步的提升的作用。</p><p><img src="/SongXJ01/images/YOLO%E7%9B%AE%E6%A0%87%E6%A3%80%E6%B5%8B%E7%AE%97%E6%B3%95%E6%B1%87%E6%80%BB/image_261WFN5zom.png" alt="YOLOv4 的效果"></p><h3 id="修改骨干网络为 -cspdarknet53">修改骨干网络为 CSPDarknet53</h3><p>  借鉴了 2019CSPNet 的经验，并结合先前的 Darkent53，获得了新的骨干网络 CSPDarknet53。在 CSPNet 中，存在如下操作，即：进入每个 stage 先将数据划分为两部分，如下图中的 part1、part2，区别在于 CSPNet 中直接对通道维度进行划分，而 YOLOv4 应用时是利用两个 1x1 卷积层来实现的。两个分支的信息在交汇处进行 Concat 拼接。</p><p><img src="/SongXJ01/images/YOLO%E7%9B%AE%E6%A0%87%E6%A3%80%E6%B5%8B%E7%AE%97%E6%B3%95%E6%B1%87%E6%80%BB/image_y1XwtlmUpj.png" alt="CSPDarknet53"></p><h3 id="引入 spp 空间金字塔池化模块">引入 spp 空间金字塔池化模块</h3><p>  引入 SPP 结构来增加感受野，采用 1x1、5x5、9x9、13x13 的最大池化的方式，进行多尺度融合，输出按照通道进行 concat 融合。</p><p>  由于 CNN 网络后面接的全连接层需要固定的输入大小，故往往通过将输入图像 resize 到固定大小的方式输入卷积网络，这会造成几何失真影响精度。SPP 模块就解决了这一问题，他通过三种尺度的池化，将任意大小的特征图固定为相同长度的特征向量，传输给全连接层。因为卷积层后面的全连接层的结构是固定的。但在现实中，我们的输入的图像尺寸总是不能满足输入时要求的大小，然而通常的手法就是裁剪 (crop) 和拉伸(warp)，但这样做总归是不好的，其扭曲了原始的特征。而 SPP 层通过将候选区的特征图划分为多个不同尺寸的网格，然后对每个网格内都做最大池化，这样依旧可以让后面的全连接层得到固定的输入。</p><p><img src="/SongXJ01/images/YOLO%E7%9B%AE%E6%A0%87%E6%A3%80%E6%B5%8B%E7%AE%97%E6%B3%95%E6%B1%87%E6%80%BB/image_HZM9ceelJG.png" alt=""></p><h3 id="在 neck 部分采用 fpn-pan 的结构"><strong>在 Neck 部分采用 FPN+PAN 的结构</strong></h3><p>  借鉴 2018 年图像分割领域 PANet, 相比于原始的 PAN 结构，YOLOV4 实际采用的 PAN 结构将 addition 的方式改为了 concatenation。如下图：</p><p><img src="/SongXJ01/images/YOLO%E7%9B%AE%E6%A0%87%E6%A3%80%E6%B5%8B%E7%AE%97%E6%B3%95%E6%B1%87%E6%80%BB/image_O9U0izUx4Y.png" alt=""></p><p>  由于 FPN 结构是自顶向下的，将高级特征信息以上采样的方式向下传递，但是融合的信息依旧存在不足，因此 YOLOv4 在 FPN 之后又添加了 PAN 结构，再次将信息从底部传递到顶部，如此一来，FPN 自顶向下传递强语义信息，而 PAN 则自底向上传递强定位信息，达到更强的特征聚合效果。</p><p>  整个 NECK 结构如下图所示：</p><p><img src="/SongXJ01/images/YOLO%E7%9B%AE%E6%A0%87%E6%A3%80%E6%B5%8B%E7%AE%97%E6%B3%95%E6%B1%87%E6%80%BB/image_FsjBe7If49.png" alt=""></p><p><br><br></p><h2 id="yolov5">YOLOv5</h2><p><img src="/SongXJ01/images/YOLO%E7%9B%AE%E6%A0%87%E6%A3%80%E6%B5%8B%E7%AE%97%E6%B3%95%E6%B1%87%E6%80%BB/d2f4aa5f2f69400b8e680c55760ad5f2_4SKImTYBPa.png" alt="YOLOv5 网络结构图"></p><p>  2020 年 2 月 YOLO 之父 Joseph Redmon 宣布退出计算机视觉研究领域，2020 年 4 月 23 日 YOLOv4 发布，2020 年 6 月 10 日 YOLOv5 发布。</p><ul><li>使用 Pytorch 框架，对用户非常友好，能够方便地训练自己的数据集，相对于 YOLO v4 采用的 Darknet 框架，Pytorch 框架更容易投入生产。</li><li>代码易读，整合了大量的计算机视觉技术，非常有利于学习和借鉴。</li><li>能够轻松的将 Pytorch 权重文件转化为安卓使用的 ONXX 格式，然后可以转换为 OpenCV 的使用格式，或者通过 CoreML 转化为 IOS 格式，直接部署到手机应用端。</li><li>有非常轻量级的模型大小， YOLOv5 的大小仅有 27MB ， 使用 Darknet 架构的 YOLOv4 有 244MB。</li></ul><h3 id="使用 sppf 结构代替了 spp">使用 SPPF 结构代替了 SPP</h3><p>  主要区别就是 MaxPool 由原来的并行调整为了串行，值得注意的是：串行两个 5 x 5 大小的 MaxPool 和一个 9 x 9 大小的 MaxPool 是等价的，串行三个 5 x 5 大小的 MaxPool 层和一个 13 x 13 大小的 MaxPool 是等价的。虽然并行和串行的效果一样，但是串行的效率更高，降低了耗时。</p><p><img src="/SongXJ01/images/YOLO%E7%9B%AE%E6%A0%87%E6%A3%80%E6%B5%8B%E7%AE%97%E6%B3%95%E6%B1%87%E6%80%BB/image_mZbM7N3E_B.png" alt=""></p><p><img src="/SongXJ01/images/YOLO%E7%9B%AE%E6%A0%87%E6%A3%80%E6%B5%8B%E7%AE%97%E6%B3%95%E6%B1%87%E6%80%BB/image__NkT1SV9e7.png" alt=""></p><h3 id="focus 操作">Focus 操作</h3><p><img src="/SongXJ01/images/YOLO%E7%9B%AE%E6%A0%87%E6%A3%80%E6%B5%8B%E7%AE%97%E6%B3%95%E6%B1%87%E6%80%BB/image_1bR38WssFU.png" alt="Focus 操作"></p><p><br><br></p><h2 id="yolox">YOLOX</h2><p><img src="/SongXJ01/images/YOLO%E7%9B%AE%E6%A0%87%E6%A3%80%E6%B5%8B%E7%AE%97%E6%B3%95%E6%B1%87%E6%80%BB/3fd25ff946474c6b8a51b47cd2c240bd_IWu_YJdvCV.png" alt="YOLOX 网络结构图 -1"></p><p><img src="/SongXJ01/images/YOLO%E7%9B%AE%E6%A0%87%E6%A3%80%E6%B5%8B%E7%AE%97%E6%B3%95%E6%B1%87%E6%80%BB/image_W9BH6ePYx9.png" alt="YOLOX 网络结构图 -2"></p><ul><li>一般情况下，可以选择<strong>Yolox-Nano、Yolox-Tiny、Yolox-s 用于移动端部署</strong>。</li><li><strong>Yolox-m、Yolox-l、Yolox-x 用于 GPU 服务器部署</strong></li></ul><p>  YOLO 检测器调整为了 Anchor-Free 形式并集成了其他先进检测技术（比如 decoupled head、label assignment SimOTA）取得了 SOTA 性能，比如：</p><p>  具有与 YOLOv4-CSP、YOLOv5-L 相当的参数量，YOLOX-L 取得了 50.0%AP 指标同时具有 68.9fps 推理速度，指标超过 YOLOv5-L 1.8%;</p><p>  值得一提的是，作者使用的 baseline 是 YOLOv3 + DarkNet53（所谓的 YOLOv3-SSP）</p><h3 id="1-decoupled-head- 解耦头">1. Decoupled Head 解耦头</h3><p>  检测头耦合会影响模型性能。采用解耦头替换 YOLO 的检测头可以显著改善模型收敛速度。解耦头结构考虑到 <strong>分类 </strong>和 <strong>定位</strong> 所关注的内容的不同。<br>同时为了避免计算量的大量增加，YOLOX 的 Decoupled Head 结构（轻量解耦头），会先进行 1x1 的降维操作，然后再接上分类和定位两个并行分支（均为 3 × 3 卷积）。</p><p><img src="/SongXJ01/images/YOLO%E7%9B%AE%E6%A0%87%E6%A3%80%E6%B5%8B%E7%AE%97%E6%B3%95%E6%B1%87%E6%80%BB/image_Liqa3kRe4J.png" alt=""></p><h3 id="2-mosaic-mixup- 数据增强">2. Mosaic + Mixup 数据增强</h3><ul><li><p><strong>Mosaic 数据增强</strong>：基本原理就是在训练集中随机选择若干个（一般是 4 个）图像，经过裁剪拼接形成新的训练集元素，可以缓解训练集元素少或者增强识别能力。</p></li><li><p>Mixup 数据增强： 对图像进行混类增强的算法，它将不同类之间的图像进行混合，从而扩充训练数据集</p></li></ul><p><img src="/SongXJ01/images/YOLO%E7%9B%AE%E6%A0%87%E6%A3%80%E6%B5%8B%E7%AE%97%E6%B3%95%E6%B1%87%E6%80%BB/image_xmL0MKtaB8.png" alt=""></p><h3 id="3-anchor-free">3. Anchor-free</h3><p>  YOLOv4、YOLOv5 均采用了 YOLOv3 原始的 anchor 设置。然而 anchor 机制存在诸多问题：</p><p>(1) 为获得最优检测性能，需要在训练之前进行聚类分析以确定最佳 anchor 集合，这些 anchor 集合存在数据相关性，泛化性能较差；<br>(2) anchor 机制提升了检测头的复杂度。</p><p>  将 YOLO 转换为 anchor-free 形式非常简单，我们将每个位置的预测从 3 下降为 1 并直接预测四个值：即两个 offset 以及高宽。</p><p><a target="_blank" rel="noopener" href="https://blog.csdn.net/SMF0504/article/details/109214527">AnchorFree 系列算法详解</a></p><h3 id="4-multi-positives">4. Multi positives</h3><p>  为确保与 YOLOv3 的一致性，前述 anchor-free 版本仅仅对每个目标赋予一个正样本，而忽视了其他高质量预测。参考 FCOS，我们简单的赋予中心 3×3 区域为正样本。</p><h3 id="5-simota">5. SimOTA</h3><p>  SimOTA 的作用是为不同目标设定不同的正样本数量，例如蚂蚁和西瓜，传统的正样本分配方案常常为同一场景下的西瓜和蚂蚁分配同样的正样本数，那要么蚂蚁有很多低质量的正样本，要么西瓜仅仅只有一两个正样本。对于哪个分配方式都是不合适的。</p><h3 id="6- 主干网络 -cspdarknet- 加入 fcous 结构">6. 主干网络（CSPDarknet）加入 Fcous 结构</h3><p>  主干网络加入 Fcous 结构，将图片宽高信息缩小，减小参数量，提升网络计算速度</p><p>  Fcous 结构：将输入的图片先经过 Fcos 结构对图片进行每隔一个像素取出一个值，得到四个特征层，然后再进行 concat。从而图片宽高的信息缩小，通道数增加。在原始信息丢失较少的情况下，减小了参数量。</p><p><img src="/SongXJ01/images/YOLO%E7%9B%AE%E6%A0%87%E6%A3%80%E6%B5%8B%E7%AE%97%E6%B3%95%E6%B1%87%E6%80%BB/image_C5q9dlW-Bg.png" alt=""></p><h3 id="7-silu 激活函数">7. <strong>SiLU 激活函数</strong></h3><p>  <strong>SiLU 函数 </strong>相比于 <strong>ReLU</strong> 非线性能力更强，同时继承了 <strong>ReLU</strong> 收敛更快的优点。</p><p><img src="/SongXJ01/images/YOLO%E7%9B%AE%E6%A0%87%E6%A3%80%E6%B5%8B%E7%AE%97%E6%B3%95%E6%B1%87%E6%80%BB/image_g1bv8gGFv9.png" alt=""></p><p><br><br></p><h2 id="yolov6">YOLOv6</h2><p><a target="_blank" rel="noopener" href="https://blog.csdn.net/qq_34795071/article/details/125442065">YOLOv6 网络结构图</a></p><p><img src="/SongXJ01/images/YOLO%E7%9B%AE%E6%A0%87%E6%A3%80%E6%B5%8B%E7%AE%97%E6%B3%95%E6%B1%87%E6%80%BB/a0510238a9404914a6d341c0575973ad_yNpGFYrQkO.png" alt=""></p><p>  YOLOv6 是由美团推出的，所做的主要工作是为了更加适应 GPU 设备，将 2021 年的 RepVGG 结构引入到了 YOLO。YOLOv6 主要在 Backbone、Neck、Head 以及训练策略等方面进行了诸多的改进：</p><ul><li>统一设计了更高效的 Backbone 和 Neck：受到硬件感知神经网络设计思想的启发，基于 RepVGG style 设计了可重参数化、更高效的骨干网络 <strong>EfficientRep Backbone</strong> 和 <strong>Rep-PAN Neck</strong>。</li><li>检测头部分模仿 YOLOX，进行了解耦操作，优化设计了更简洁有效的 Efficient Decoupled Head，进一步降低了一般解耦头带来的额外延时开销。</li><li>在训练策略上，采用 Anchor-free 无锚范式，同时辅以 SimOTA 标签分配策略以及 SIoU 边界框回归损失来进一步提高检测精度。</li></ul><h3 id="1- 更适应 gpu 的骨干网络设计">1. 更适应 GPU 的骨干网络设计</h3><p>  YOLOv5/YOLOX 使用的 Backbone 和 Neck 都基于 CSPNet 搭建，采用了多分支的方式和残差结构。对于 GPU 等硬件来说，这种结构会一定程度上增加延时，同时减小内存带宽利用率。按照 RepVGG 的思路，为每一个 3x3 的卷积添加平行了一个 1x1 的卷积分支和恒等映射分支，然后在推理时融合为 3x3 的结构，这种方式对计算密集型的硬件设备会比较友好。</p><p><img src="/SongXJ01/images/YOLO%E7%9B%AE%E6%A0%87%E6%A3%80%E6%B5%8B%E7%AE%97%E6%B3%95%E6%B1%87%E6%80%BB/image_vc92ntzriY.png" alt=""></p><h3 id="2- 简洁高效的 -decoupled-head">2. 简洁高效的 Decoupled Head</h3><p>  在 YOLOv6 中，采用了解耦检测头（Decoupled Head）结构，并对其进行了精简设计。原始 YOLOv5 的检测头是通过分类和回归分支融合共享的方式来实现的，而 YOLOX 的检测头则是将分类和回归分支进行解耦，同时新增了两个额外的 3x3 的卷积层，虽然提升了检测精度，但一定程度上增加了网络延时。</p><p><img src="/SongXJ01/images/YOLO%E7%9B%AE%E6%A0%87%E6%A3%80%E6%B5%8B%E7%AE%97%E6%B3%95%E6%B1%87%E6%80%BB/image_zoR2HNFoDr.png" alt=""></p><h3 id="3-anchor-free- 无锚范式">3. <strong>Anchor-free 无锚范式</strong></h3><p>  YOLOv6 采用了更简洁的 Anchor-free 检测方法。由于 Anchor-based 检测器需要在训练之前进行聚类分析以确定最佳 Anchor 集合，这会一定程度提高检测器的复杂度；同时，在一些边缘端的应用中，需要在硬件之间搬运大量检测结果的步骤，也会带来额外的延时。而 Anchor-free 无锚范式因其泛化能力强，解码逻辑更简单，在近几年中应用比较广泛。经过对 Anchor-free 的实验调研，我们发现，相较于 Anchor-based 检测器的复杂度而带来的额外延时，Anchor-free 检测器在速度上有 51% 的提升。</p><h3 id="4-simota- 标签分配策略">4. <strong>SimOTA 标签分配策略</strong></h3><p>  为了获得更多高质量的正样本，YOLOv6 引入了 SimOTA 算法动态分配正样本，进一步提高检测精度。YOLOv5 的标签分配策略是基于 Shape 匹配，并通过跨网格匹配策略增加正样本数量，从而使得网络快速收敛，但是该方法属于静态分配方法，并不会随着网络训练的过程而调整。</p><h3 id="5-siou- 边界框回归损失">5. <strong>SIoU 边界框回归损失</strong></h3><p>  为了进一步提升回归精度，YOLOv6 采用了 SIoU 边界框回归损失函数来监督网络的学习。</p><p>  SIoU 损失函数由 4 个 Cost 函数组成：</p><ul><li>Angle cost 角度</li><li>Distance cost 距离</li><li>Shape cost 形状</li><li>IoU cost 重合度</li></ul><p><br><br></p><h2 id="yolov7">YOLOv7</h2><p>  官方版的 YOLOv7 相同体量下比 YOLOv5 精度更高，速度快 120%（FPS），比 YOLOX 快 180%（FPS）。YOLOv7 依旧基于 anchor based 的方法，同时在网络架构上增加 E-ELAN 层，并将 REP 层也加入进来，方便后续部署，同时在训练时，在 head 时，新增 Aux_detect 用于辅助检测，对预测结果的一种初筛，有种 two-stage 的感觉。</p><p><img src="/SongXJ01/images/YOLO%E7%9B%AE%E6%A0%87%E6%A3%80%E6%B5%8B%E7%AE%97%E6%B3%95%E6%B1%87%E6%80%BB/f8a6ccbd93094b548804bc64b46468df_ZsJIM3i-zZ.png" alt=""></p><p>  根据上图的架构图走一遍网络流程：先对输入的图片预处理，对齐成 640*640 大小的 RGB 图片，输入到 backbone 网络中，根据 backbone 网络中的三层输出，在 head 层通过 backbone 网络继续输出三层不同 size 大小的<strong>feature map</strong>，经过 RepVGG block 和 conv，对图像检测的三类任务（分类、前后背景分类、边框）预测，输出最后的结果。</p><p>  YOLOv7 因为基于 anchor based 的目标检测，与 YOLOv5 相同，YOLOv6 的正负样本的匹配策略则与 YOLOX 相同，YOLOv7 则基本集成两家之所长。</p><p>  YOLOv7 大部分继承自 YOLOv5，包括整体网络架构、配置文件的设置和训练、推理、验证过程等等；此外，YOLOv7 也有不少继承自 YOLOR，毕竟是同一个作者前后年的工作，包括不同网络的设计、超参数设置以及隐性知识学习的加入；还有就是在正样本匹配时仿照了 YOLOX 的 SimOTA 策略。</p><p><strong>优势：参数量和计算量大幅度减少</strong>。</p><h3 id="1- 正样本分配策略">1. 正样本分配策略</h3><ul><li>中心点增加了一个 0.5 个单位的偏移扩散，提升召回。</li><li>为了让正样本更多</li><li>正样本分配，IoU 计算，通过累加和动态筛选正样本</li></ul><p><img src="/SongXJ01/images/YOLO%E7%9B%AE%E6%A0%87%E6%A3%80%E6%B5%8B%E7%AE%97%E6%B3%95%E6%B1%87%E6%80%BB/image_uITEYuxEGh.png" alt=""></p><p>  YOLOv7 的标签分配策略（正样本筛选），集成了 YOLOv5 和 YOLOX 两者的精华：</p><ul><li><p><strong>YOLOv5</strong></p><p>Step1：Autoanchor 策略，获得数据集最佳匹配的 9 个 anchor（可选）</p><p>Step2：根据 GT 框与 anchor 的宽高比，过滤掉不合适的 anchor</p><p>Step3：选择 GT 框的中心网格以及最邻近的 2 个邻域网格作为正样本筛选区域（辅助头则选择周围 4 个邻域网格）</p></li></ul><p><img src="/SongXJ01/images/YOLO%E7%9B%AE%E6%A0%87%E6%A3%80%E6%B5%8B%E7%AE%97%E6%B3%95%E6%B1%87%E6%80%BB/image_ZtqkIEEhGJ.png" alt=""></p><ul><li><p><strong>YOLOX</strong></p><p>Step4：计算 GT 框与正样本 IOU 并从大到小排序，选取前 10 个值进行求和（P6 前 20 个），并取整作为当前 GT 框的 K 值</p><p>Step5：根据损失函数计算每个 GT 框和候选 anchor 的损失，保留损失最小的前 K 个</p><p>Step6：去掉同一个 anchor 被分配到多个 GT 框的情况</p></li></ul><h3 id="2-sppcspc 模块">2. SPPCSPC 模块</h3><p><a target="_blank" rel="noopener" href="https://yolov5.blog.csdn.net/article/details/126354660?spm=1001.2101.3001.6661.1&amp;utm_medium=distribute.pc_relevant_t0.none-task-blog-2~default~CTRLIST~Rate-1-126354660-blog-126531046.pc_relevant_recovery_v2&amp;depth_1-utm_source=distribute.pc_relevant_t0.none-task-blog-2~default~CTRLIST~Rate-1-126354660-blog-126531046.pc_relevant_recovery_v2&amp;utm_relevant_index=1">https://yolov5.blog.csdn.net/article/details/126354660</a></p><p>  总的输入会被分成三段进入不同的分支，最中间的分支其实就是金字塔池化操作，左侧分支类似于 depthwise conv，但是请注意，中间的 3×3 卷积并未进行分组，依旧是标准卷积，右侧则为一个 point conv，最后将所有分支输出的信息流进行 concat。</p><p><img src="/SongXJ01/images/YOLO%E7%9B%AE%E6%A0%87%E6%A3%80%E6%B5%8B%E7%AE%97%E6%B3%95%E6%B1%87%E6%80%BB/image_6RGn2lC09Q.png" alt=""></p><h3 id="3-e-elan 模块">3. E-ELAN 模块</h3><p><img src="/SongXJ01/images/YOLO%E7%9B%AE%E6%A0%87%E6%A3%80%E6%B5%8B%E7%AE%97%E6%B3%95%E6%B1%87%E6%80%BB/image_jFVY-Qwc0g.png" alt=""></p><p>  E-ELAN 只改变了计算模块中的结构，而过渡层的结构则完全不变。作者的策略是利用分组卷积来扩展计算模块的通道和基数，将相同的 group parameter 和 channel multiplier 用于计算每一层中的所有模块。然后，将每个模块计算出的特征图根据设置的分组数打乱成 G 组，最后将它们连接在一起。此时，每一组特征图中的通道数将与原始体系结构中的通道数相同。最后，作者添加了 G 组特征来 merge cardinality。除了维护原始的 ELAN 设计架构外，E-ELAN 还可以指导不同的分组模块来学习更多样化的特性。</p><h3 id="4- 辅助头检测">4. 辅助头检测</h3><p>  常用的方式是图（c）所示，即辅助头和引导头各自独立，分别利用 ground truth 和它们（辅助头、引导头）各自的预测结果实现标签分配。YOLOV7 算法中提出了利用引导头的预测结果作为指导，生成从粗到细的层次标签，将这些层次标签分别用于辅助头和引导头的学习，如下图（d）和（e）所示。</p><p><img src="/SongXJ01/images/YOLO%E7%9B%AE%E6%A0%87%E6%A3%80%E6%B5%8B%E7%AE%97%E6%B3%95%E6%B1%87%E6%80%BB/image_dkzANMbGMQ.png" alt=""></p><h3 id="5- 复合模型缩放">5. 复合模型缩放</h3><p>  类似于 YOLOv5、Scale YOLOv4、YOLOX，一般是对 depth、width 或者 module scale 进行缩放，实现扩大或缩小 baseline 的目的。</p><p><img src="/SongXJ01/images/YOLO%E7%9B%AE%E6%A0%87%E6%A3%80%E6%B5%8B%E7%AE%97%E6%B3%95%E6%B1%87%E6%80%BB/image_hGrBsf_uYx.png" alt=""></p><h3 id="6- 卷积重参化">6. <strong>卷积重参化</strong></h3><p>  <strong>引入了卷积重参化并进行了改进 </strong>采用梯度传播路径来分析不同的重参化模块应该和哪些网络搭配使用。同时分析出 RepConv 中的 identity 破坏了 ResNet 中的残差结构和 DenseNet 中的跨层连接，因此作者做了改进，采用没有 Identity 连接的 RepConv 结构进行卷积重参数化。下图是设计的用于 PlainNet 和 ResNet 的计划重参数卷积。</p><p><img src="/SongXJ01/images/YOLO%E7%9B%AE%E6%A0%87%E6%A3%80%E6%B5%8B%E7%AE%97%E6%B3%95%E6%B1%87%E6%80%BB/image_P-a1KGH305.png" alt=""></p><h3 id="yolov7 基础版本的区别">YOLOv7 基础版本的区别</h3><p>  YOLOv7 基础版本有三种，分别是 YOLOv7、YOLOv7-tiny 和 YOLOv7-W6：</p><ul><li><strong>YOLOv7</strong>是针对普通 GPU 计算优化的基础模型。</li><li><strong>YOLOv7-tiny</strong>是针对边缘 GPU 优化的基础模型。计算机视觉模型的后缀“小”意味着它们针对边缘 AI 和深度学习工作负载进行了优化，并且更轻量级，可以在移动计算设备或分布式边缘服务器和设备上运行 ML。该模型对于分布式现实世界的计算机视觉应用程序很重要。与其他版本相比，边缘优化的 YOLOv7-tiny 使用 leaky ReLU 作为激活函数，而其他模型使用 SiLU 作为激活函数。</li><li><strong>YOLOv7-W6</strong>是针对云 GPU 计算优化的基础模型。此类云图形单元 (GPU) 是用于运行应用程序以在云中处理大量 AI 和深度学习工作负载的计算机实例，而无需在本地用户设备上部署 GPU。</li></ul><p><br><br></p><hr><br><p></p><p><br><br></p><h2 id="正负样本匹配策略对比">正负样本匹配策略对比</h2><h3 id="yolov5 的正负样本匹配策略">YOLOv5 的正负样本匹配策略</h3><p>  YOLOv5 基于 anchor based，在开始训练前，会基于训练集中 gt 框，通过 k-means 聚类算法，先验获得 9 个从小到大排列的 anchor 框。先将每个 gt 与 9 个 anchor 匹配（以前是 IOU 匹配，yolov5 中变成 shape 匹配，计算 gt 与 9 个 anchor 的长宽比，如果长宽比小于设定阈值，说明该 gt 和对应的 anchor 匹配）</p><p>  YOLOv5 有三层网络，9 个 anchor, 从小到大，每 3 个 anchor 对应一层 prediction 网络，gt 与之对应 anchor 所在的层，用于对该 gt 做训练预测，一个 gt 可能与几个 anchor 均能匹配上。<br>所以一个 gt 可能在不同的网络层上做预测训练，大大增加了正样本的数量，当然也会出现 gt 与所有 anchor 都匹配不上的情况，这样 gt 就会被当成背景，不参与训练，说明 anchor 框尺寸设计的不好。</p><p>  在训练过程中怎么定义正负样本呢，因为 yolov5 中负样本不参与训练，所以要增加正样本的数量。gt 框与 anchor 框匹配后，得到 anchor 框对应的网络层的 grid，看 gt 中心点落在哪个 grid 上，不仅取该 grid 中和 gt 匹配的 anchor 作为正样本，还取相邻的的两个 grid 中的 anchor 为正样本。</p><p>  如下图所示， <strong>绿色的 gt 框中心点落在红色 grid 的第三象限里，那不仅取该 grid, 还要取左边的 grid 和下面的 grid，</strong> 这样基于三个 grid 和匹配的 anchor 就有三个中心点位于三个 grid 中心点，长宽为 anchor 长宽的正样本，同时 gt 不仅与一个 anchor 框匹配，如果跟几个 anchor 框都匹配上，所以可能有 3-27 个正样本，增大正样本数量。</p><p><img src="/SongXJ01/images/YOLO%E7%9B%AE%E6%A0%87%E6%A3%80%E6%B5%8B%E7%AE%97%E6%B3%95%E6%B1%87%E6%80%BB/image_o0omt9JvWT.png" alt=""></p><h3 id="yolov6 的正负样本匹配策略">YOLOv6 的正负样本匹配策略</h3><p>  YOLOv6 的正负样本匹配策略同 YOLOX，YOLOX 因为是 anchor free，anchor free 因为缺少先验框这个先验知识，理论上应该是对场景的泛化性更好，同时参见旷视的官方解读：Anchor 增加了检测头的复杂度以及生成结果的数量，将大量检测结果从 NPU 搬运到 CPU 上对于某些边缘设备是无法容忍的。</p><p>  OLOv6 中的正样本筛选，主要分成以下几个部分：<br>①：基于两个维度来粗略筛选；<br>②：基于 simOTA 进一步筛选。<br>具体步骤如下：</p><p><img src="/SongXJ01/images/YOLO%E7%9B%AE%E6%A0%87%E6%A3%80%E6%B5%8B%E7%AE%97%E6%B3%95%E6%B1%87%E6%80%BB/image_Y3TQa7VFVz.png" alt=""></p><p>  tie 标签的 gt 如图所示，找到 gt 的中心点（Cx,Cy）, 计算中心点到左上角的距离（l_l,l_t）, 右下角坐标（l_r,l_b）, 然后从两步筛选正样本：</p><p>  第一步粗略筛选第一个维度是如果 grid 的中心点落在 gt 中，则认为该 grid 所预测的框为正样本，如图所示的红色和橙色部分 <strong>，第二个维度是 </strong>以 gt 的中心点所在 grid 的中心点为中心点，上下左右扩充 2.5 个 grid 步长范围内的 grid，则默认该 grid 所预测的框为正样本，如图紫色和橙色部分。</p><p><img src="/SongXJ01/images/YOLO%E7%9B%AE%E6%A0%87%E6%A3%80%E6%B5%8B%E7%AE%97%E6%B3%95%E6%B1%87%E6%80%BB/image_XY1F6R5rc3.png" alt=""></p><p>第二步：通过 SimOTA 进一步筛选：</p><p>  SimOTA 流程如下：<br>①计算初筛正样本与 gt 的 IOU，并对 IOU 从大到小排序，取前十之和并取整, 记为 b。<br>②计算初筛正样本的 cos 代价函数，将 cos 代价函数从小到大排列，取 cos 前 b 的样本为正样本。<br>同时考虑同一个 grid 预测框被两个 gt 关联的情况，取 cos 较小的值，该预测框为对应的 gt 的正样本。</p><h3 id="yolov7 的正负样本匹配策略">YOLOv7 的正负样本匹配策略</h3><p>  YOLOv7 因为基于 anchor based , 集成 v5 和 v6 两者的精华，即 YOLOv6 中的第一步的初筛换成了 YOLOv5 中的筛选正样本的策略，保留第二步的 simOTA 进一步筛选策略。</p><p>  同时 YOLOv7 中有 aux_head 和 lead_head 两个 head ,aux_head 做为辅助，其筛选正样本的策略和 lead_head 相同，但更宽松。如在第一步筛选时，lead_head 取中心点所在 grid 和与之接近的两个 grid 对应的预测框做为正样本，如图绿色的 grid，aux_head 则取中心点以及周围的 4 个预测框为正样本。如下图绿色＋蓝色区域的 grid。</p><p><img src="/SongXJ01/images/YOLO%E7%9B%AE%E6%A0%87%E6%A3%80%E6%B5%8B%E7%AE%97%E6%B3%95%E6%B1%87%E6%80%BB/image_5XPTEswQNp.png" alt=""></p><p>  同时在第二步 SimOTA 部分，lead_head 是计算初筛正样本与 gt 的 IOU，并对 IOU 从大到小排序，取前十之和并取整，记为 b。aux_head 则取前二十之和并取整。其他步骤相同，aux_head 主要是为了增加召回率，防止漏检，lead_head 再基于 aux_head 做进一步筛选。</p><p><br><br></p><h2 id="yolov5- 和 -yolox 的对比">YOLOv5 和 YOLOX 的对比</h2><p><a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/397993315">深入浅出 Yolo 系列之 Yolox 核心基础完整讲解</a></p><p><strong>YOLOv5：</strong></p><p><img src="/SongXJ01/images/YOLO%E7%9B%AE%E6%A0%87%E6%A3%80%E6%B5%8B%E7%AE%97%E6%B3%95%E6%B1%87%E6%80%BB/image_97fBqdMwff.png" alt=""></p><p><strong>YOLOX：</strong></p><p><img src="/SongXJ01/images/YOLO%E7%9B%AE%E6%A0%87%E6%A3%80%E6%B5%8B%E7%AE%97%E6%B3%95%E6%B1%87%E6%80%BB/image_JR25nDFTWY.png" alt=""></p><p>由上面两张图的对比，及前面的内容可以看出，<strong>Yolov5s 和 Yolox-s 主要区别 </strong>在于：</p><p><strong>（1）输入端：</strong> 在 Mosa 数据增强的基础上，增加了 Mixup 数据增强效果；</p><p><strong>（2）Backbone：</strong> 激活函数采用 SiLU 函数；</p><p><strong>（3）Neck：</strong> 激活函数采用 SiLU 函数；</p><p><strong>（4）输出端：</strong> 检测头改为 Decoupled Head、采用 anchor free、multi positives、SimOTA 的方式。</p><p>在前面 Yolov3 baseline 的基础上，以上的 tricks，取得了很不错的涨点。</p><p><br><br></p><h2 id="参考">参考</h2><ul><li><a target="_blank" rel="noopener" href="https://rockyding.blog.csdn.net/article/details/107199675?spm=1001.2101.3001.6650.4&amp;utm_medium=distribute.pc_relevant.none-task-blog-2~default~CTRLIST~Rate-4-107199675-blog-126392748.pc_relevant_3mothn_strategy_and_data_recovery&amp;depth_1-utm_source=distribute.pc_relevant.none-task-blog-2~default~CTRLIST~Rate-4-107199675-blog-126392748.pc_relevant_3mothn_strategy_and_data_recovery&amp;utm_relevant_index=9">https://rockyding.blog.csdn.net/article/details/107199675</a></li><li><a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/539932517">YOLO 家族进化史（v1-v7）</a></li></ul><p><br><br><br><br></p><div id="reword-out"><div id="reward-btn">打赏</div></div></div><div class="declare"><ul class="post-copyright"><li><i class="ri-copyright-line"></i> <strong>版权声明： </strong>本博客所有文章除特别声明外，著作权归作者所有。转载请注明出处！</li></ul></div><footer class="article-footer"><div class="share-btn"><span class="share-sns share-outer"><i class="ri-share-forward-line"></i> 分享</span><div class="share-wrap"><i class="arrow"></i><div class="share-icons"><a class="weibo share-sns" href="javascript:;" data-type="weibo"><i class="ri-weibo-fill"></i> </a><a class="weixin share-sns wxFab" href="javascript:;" data-type="weixin"><i class="ri-wechat-fill"></i> </a><a class="qq share-sns" href="javascript:;" data-type="qq"><i class="ri-qq-fill"></i> </a><a class="douban share-sns" href="javascript:;" data-type="douban"><i class="ri-douban-line"></i> </a><a class="facebook share-sns" href="javascript:;" data-type="facebook"><i class="ri-facebook-circle-fill"></i> </a><a class="twitter share-sns" href="javascript:;" data-type="twitter"><i class="ri-twitter-fill"></i> </a><a class="google share-sns" href="javascript:;" data-type="google"><i class="ri-google-fill"></i></a></div></div></div><div class="wx-share-modal"><a class="modal-close" href="javascript:;"><i class="ri-close-circle-line"></i></a><p>扫一扫，分享到微信</p><div class="wx-qrcode"><img src="//api.qrserver.com/v1/create-qr-code/?size=150x150&data=https://songxj01.github.io/SongXJ01/2023/YOLO%E7%9B%AE%E6%A0%87%E6%A3%80%E6%B5%8B%E7%AE%97%E6%B3%95%E6%B1%87%E6%80%BB/" alt="微信分享二维码"></div></div><div id="share-mask"></div><ul class="article-tag-list" itemprop="keywords"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/SongXJ01/tags/YOLO/" rel="tag">YOLO</a></li><li class="article-tag-list-item"><a class="article-tag-list-link" href="/SongXJ01/tags/%E7%9B%AE%E6%A0%87%E6%A3%80%E6%B5%8B/" rel="tag">目标检测</a></li><li class="article-tag-list-item"><a class="article-tag-list-link" href="/SongXJ01/tags/%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89/" rel="tag">计算机视觉</a></li></ul></footer></div><nav class="article-nav"><a href="/SongXJ01/2023/MySQL%E6%95%B0%E6%8D%AE%E5%BA%93%E9%83%A8%E7%BD%B2%E4%B8%8E%E6%93%8D%E4%BD%9C/" class="article-nav-link"><strong class="article-nav-caption">上一篇</strong><div class="article-nav-title">MySQL 数据库部署与操作</div></a><a href="/SongXJ01/2023/Hexo%E5%8D%9A%E5%AE%A2%E6%8A%80%E8%83%BD%E6%A0%91/" class="article-nav-link"><strong class="article-nav-caption">下一篇</strong><div class="article-nav-title">Hexo 博客技能树</div></a></nav><div id="vcomments-box"><div id="vcomments"></div></div><script src="//cdn1.lncld.net/static/js/3.0.4/av-min.js"></script><script src="https://cdn.jsdelivr.net/npm/valine@1.4.14/dist/Valine.min.js"></script><script>new Valine({
    el: "#vcomments",
    app_id: "tahglBYerISh7wrhyXMEH6UV-gzGzoHsz",
    app_key: "G2qOYTyMGB3F79hHhg6LARHs",
    path: window.location.pathname,
    avatar: "mp",
    placeholder: "欢迎交流讨论 ......",
    recordIP: true,
  });
  const infoEle = document.querySelector("#vcomments .info");
  if (infoEle && infoEle.childNodes && infoEle.childNodes.length > 0) {
    infoEle.childNodes.forEach(function (item) {
      item.parentNode.removeChild(item);
    });
  }</script><style>#vcomments-box{padding:5px 30px}@media screen and (max-width:800px){#vcomments-box{padding:5px 0}}#vcomments-box #vcomments{background-color:#fff}.v .vlist .vcard .vh{padding-right:20px}.v .vlist .vcard{padding-left:10px}</style></article></section><footer class="footer"><div class="outer"><ul><li>Copyrights &copy; 2019-2024 <i class="ri-heart-fill heart_icon"></i> SongXJ</li></ul><ul><li></li></ul><ul><li><span><span><i class="ri-user-3-fill"></i>访问人数:<span id="busuanzi_value_site_uv"></span></span> <span class="division">|</span> <span><i class="ri-eye-fill"></i>浏览次数:<span id="busuanzi_value_page_pv"></span></span></span></li></ul><ul></ul><ul></ul><ul><li><script type="text/javascript" src="https://s9.cnzz.com/z_stat.php?id=1280220642&amp;web_id=1280220642"></script></li></ul></div></footer></main><div class="float_btns"><div class="totop" id="totop"><i class="ri-arrow-up-line"></i></div><div class="todark" id="todark"><i class="ri-moon-line"></i></div></div><aside class="sidebar on"><button class="navbar-toggle"></button><nav class="navbar"><div class="logo"><a href="/SongXJ01/"><img src="/SongXJ01/images/songxj.svg" alt="SongXJ&#39;s Blog"></a></div><ul class="nav nav-main"><li class="nav-item"><a class="nav-item-link" href="/SongXJ01/">主页</a></li><li class="nav-item"><a class="nav-item-link" href="/SongXJ01/photos">摄影</a></li><li class="nav-item"><a class="nav-item-link" href="/SongXJ01/categories">分类</a></li><li class="nav-item"><a class="nav-item-link" href="/SongXJ01/tags">标签</a></li><li class="nav-item"><a class="nav-item-link" href="/SongXJ01/archives">归档</a></li><li class="nav-item"><a class="nav-item-link" href="/SongXJ01/about">关于我</a></li></ul></nav><nav class="navbar navbar-bottom"><ul class="nav"><li class="nav-item"><a class="nav-item-link nav-item-search" title="搜索"><i class="ri-search-line"></i></a></li></ul></nav><div class="search-form-wrap"><div class="local-search local-search-plugin"><input type="search" id="local-search-input" class="local-search-input" placeholder="Search..."><div id="local-search-result" class="local-search-result"></div></div></div></aside><div id="mask"></div><div id="reward"><span class="close"><i class="ri-close-line"></i></span><p class="reward-p"><i class="ri-cup-line"></i>请我喝杯咖啡吧~</p><div class="reward-box"><div class="reward-item"><img class="reward-img" src="/SongXJ01/SongXJ01/images/money_alipay.jpg"> <span class="reward-type">支付宝</span></div><div class="reward-item"><img class="reward-img" src="/SongXJ01/SongXJ01/images/money_wechat.png"> <span class="reward-type">微信</span></div></div></div><script src="/SongXJ01/js/jquery-3.6.0.min.js"></script><script src="/SongXJ01/js/lazyload.min.js"></script><script src="/SongXJ01/js/tocbot.min.js"></script><script>tocbot.init({tocSelector:".tocbot",contentSelector:".article-entry",headingSelector:"h1, h2, h3, h4, h5, h6",hasInnerContainers:!0,scrollSmooth:!0,scrollContainer:"main",positionFixedSelector:".tocbot",positionFixedClass:"is-position-fixed",fixedSidebarOffset:"auto"})</script><script src="https://cdn.jsdelivr.net/npm/jquery-modal@0.9.2/jquery.modal.min.js"></script><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/jquery-modal@0.9.2/jquery.modal.min.css"><script src="https://cdn.jsdelivr.net/npm/justifiedGallery@3.7.0/dist/js/jquery.justifiedGallery.min.js"></script><script src="/SongXJ01/dist/main.js"></script><div class="pswp" tabindex="-1" role="dialog" aria-hidden="true"><div class="pswp__bg"></div><div class="pswp__scroll-wrap"><div class="pswp__container"><div class="pswp__item"></div><div class="pswp__item"></div><div class="pswp__item"></div></div><div class="pswp__ui pswp__ui--hidden"><div class="pswp__top-bar"><div class="pswp__counter"></div><button class="pswp__button pswp__button--close" title="Close (Esc)"></button> <button class="pswp__button pswp__button--share" style="display:none" title="Share"></button> <button class="pswp__button pswp__button--fs" title="Toggle fullscreen"></button> <button class="pswp__button pswp__button--zoom" title="Zoom in/out"></button><div class="pswp__preloader"><div class="pswp__preloader__icn"><div class="pswp__preloader__cut"><div class="pswp__preloader__donut"></div></div></div></div></div><div class="pswp__share-modal pswp__share-modal--hidden pswp__single-tap"><div class="pswp__share-tooltip"></div></div><button class="pswp__button pswp__button--arrow--left" title="Previous (arrow left)"></button> <button class="pswp__button pswp__button--arrow--right" title="Next (arrow right)"></button><div class="pswp__caption"><div class="pswp__caption__center"></div></div></div></div></div><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/photoswipe@4.1.3/dist/photoswipe.min.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/photoswipe@4.1.3/dist/default-skin/default-skin.min.css"><script src="https://cdn.jsdelivr.net/npm/photoswipe@4.1.3/dist/photoswipe.min.js"></script><script src="https://cdn.jsdelivr.net/npm/photoswipe@4.1.3/dist/photoswipe-ui-default.min.js"></script><script>function viewer_init() {
        let pswpElement = document.querySelectorAll('.pswp')[0];
        let $imgArr = document.querySelectorAll(('.article-entry img:not(.reward-img)'))

        $imgArr.forEach(($em, i) => {
            $em.onclick = () => {
                // slider展开状态
                // todo: 这样不好，后面改成状态
                if (document.querySelector('.left-col.show')) return
                let items = []
                $imgArr.forEach(($em2, i2) => {
                    let img = $em2.getAttribute('data-idx', i2)
                    let src = $em2.getAttribute('data-target') || $em2.getAttribute('src')
                    let title = $em2.getAttribute('alt')
                    // 获得原图尺寸
                    const image = new Image()
                    image.src = src
                    items.push({
                        src: src,
                        w: image.width || $em2.width,
                        h: image.height || $em2.height,
                        title: title
                    })
                })
                var gallery = new PhotoSwipe(pswpElement, PhotoSwipeUI_Default, items, {
                    index: parseInt(i)
                });
                gallery.init()
            }
        })
    }
    viewer_init()</script><script type="text/x-mathjax-config">MathJax.Hub.Config({
      tex2jax: {
          inlineMath: [ ['$','$'], ["\\(","\\)"]  ],
          processEscapes: true,
          skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code']
      }
  });

  MathJax.Hub.Queue(function() {
      var all = MathJax.Hub.getAllJax(), i;
      for(i=0; i < all.length; i += 1) {
          all[i].SourceElement().parentNode.className += ' has-jax';
      }
  });</script><script src="https://cdn.jsdelivr.net/npm/mathjax@2.7.6/unpacked/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script><script>var ayerConfig={mathjax:!0}</script><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.11.1/dist/katex.min.css"><script src="https://cdn.jsdelivr.net/npm/katex@0.11.1/dist/katex.min.js"></script><script src="https://cdn.jsdelivr.net/npm/katex@0.11.1/dist/contrib/auto-render.min.js"></script><script src="https://cdn.jsdelivr.net/npm/katex@0.11.1/dist/contrib/copy-tex.min.js"></script><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.11.1/dist/contrib/copy-tex.min.css"><script src="/SongXJ01/js/busuanzi-2.3.pure.min.js"></script><link rel="stylesheet" href="/SongXJ01/css/clipboard.css"><script src="https://cdn.jsdelivr.net/npm/clipboard@2/dist/clipboard.min.js"></script><script>function wait(callback, seconds) {
    var timelag = null;
    timelag = window.setTimeout(callback, seconds);
  }
  !function (e, t, a) {
    var initCopyCode = function(){
      var copyHtml = '';
      copyHtml += '<button class="btn-copy" data-clipboard-snippet="">';
      copyHtml += '<i class="ri-file-copy-2-line"></i><span>COPY</span>';
      copyHtml += '</button>';
      $(".highlight .code pre").before(copyHtml);
      $(".article pre code").before(copyHtml);
      var clipboard = new ClipboardJS('.btn-copy', {
        target: function(trigger) {
          return trigger.nextElementSibling;
        }
      });
      clipboard.on('success', function(e) {
        let $btn = $(e.trigger);
        $btn.addClass('copied');
        let $icon = $($btn.find('i'));
        $icon.removeClass('ri-file-copy-2-line');
        $icon.addClass('ri-checkbox-circle-line');
        let $span = $($btn.find('span'));
        $span[0].innerText = 'COPIED';
        
        wait(function () { // 等待两秒钟后恢复
          $icon.removeClass('ri-checkbox-circle-line');
          $icon.addClass('ri-file-copy-2-line');
          $span[0].innerText = 'COPY';
        }, 2000);
      });
      clipboard.on('error', function(e) {
        e.clearSelection();
        let $btn = $(e.trigger);
        $btn.addClass('copy-failed');
        let $icon = $($btn.find('i'));
        $icon.removeClass('ri-file-copy-2-line');
        $icon.addClass('ri-time-line');
        let $span = $($btn.find('span'));
        $span[0].innerText = 'COPY FAILED';
        
        wait(function () { // 等待两秒钟后恢复
          $icon.removeClass('ri-time-line');
          $icon.addClass('ri-file-copy-2-line');
          $span[0].innerText = 'COPY';
        }, 2000);
      });
    }
    initCopyCode();
  }(window, document);</script><script>window.mermaid&&mermaid.initialize({theme:"dark"})</script></div><script data-pjax>var parent,child;document.getElementById("recent-posts")&&"/"==location.pathname&&(parent=document.getElementById("recent-posts"),child='<div class="recent-post-item" style="height: auto;width: 100%"><div class="blog-slider swiper-container-fade swiper-container-horizontal" id="swiper_container"><div class="blog-slider__wrp swiper-wrapper" style="transition-duration: 0ms"></div><div class="blog-slider__pagination swiper-pagination-clickable swiper-pagination-bullets"></div></div></div>',console.log("已挂载swiper"),parent.insertAdjacentHTML("afterbegin",child))</script><script data-pjax src="https://cdn.jsdelivr.net/gh/Zfour/Butterfly-swiper/swiper/swiper.min.js"></script><script data-pjax src="https://cdn.jsdelivr.net/gh/Zfour/Butterfly-swiper@0.18/swiper/swiperindex.js"></script><style></style></body>